# 개념 ------------------------------------------------
샘플링 편향
- 일반적으로 훈련/테스트 세트 샘플이 고루 섞이지 않으면 발생
- 최초 리스트 생성에서 고정된 분류 시 정확도 이상

스케일
- 두 특성(x, y) 간 차이 존재

표준점수(z-score)
- 각 특성값이 평균에서 표준편차의 몇 배만큼 떨어져 있는가 
  # train_scaled = (train_input - mean) / std

결정계수(R^2)
- 회귀 모델 평가
- R^2 = 1 - [{(타겟 - 예측값)^2} 합 / {(타겟 - 평균)^2 합}]
- 평균을 겨냥할 경우 0, 예측값을 겨냥할 경우 1에 가까워진다.
  # KNR에서 knr.score 매서드 결과

과대적합
- 훈련 세트에서 점수가 높았으나 테스트 점수에서 점수가 떨어지는 경우
-> k값을 늘린다

과소적합
- 훈련/테스트 세트 모두 점수가 낮은 경우
- 모델이 너무 단순해서 발생
- 훈련/테스트 데이터 세트 크기가 너무 작은 경우 발생
-> k값을 줄인

예측 오류
- 새로운 샘플(타겟)이 데이터 세트를 벗어나면서 데이터 세트 내 최댓값들의 평균으로 출력됨

선형회귀
- 특성이 하나인 경우 특성을 가장 잘 나타내는 '직선'을 학습하는 알고리즘
- y = ax + b
  파라미터
  - 모델이 지정하는 계수
  - lr에서 기울기와 상수에 해당
  - a = lr.coef_  -> 기울기 = 계수 = 가중치
  - b = lr.intercept_

다항회귀
- 2차 방저식으로 회귀선을 곡선으로

특성공학
- 기존 특성을 활용해 새로운 특성을 만들어냄 -> 파생변수?

판다스
- 데이터분석 라이브러리

CSV파일
- ','로 나누어진 텍스트 파일

특성 개수와 선형모델의 관계
- 훈련 데이터를 강력하게 학습해 테스트 데이터셋에 영향력 없는 결과 출력
- 샘플에 비해 특성이 많은 경우

규제
- 머신러닝 모델이 훈련 세트를 너무 과도하게 학습하지 않도록 방지 -> 과적합 방지
- 선형회귀 모델의 경우 특성에 곱해지는 계수(또는 기울기)의 크기를 조절
- 규제 이전에 특성 스케일이 정규화되지 않으면 규제가 공정히 작용하지 않음 -> 전처리 후 규제 적용
- 릿지회귀
  - 계수를 제곱한 값을 기준으로 규제를 적용
  - 일반적으로 선호되는 규제
  - sklearn.linear_model 패키지지
- 라쏘회귀
  - 계수를 0으로 유도 -> 값이 0이되는 특성이 생김 -> 유용한 특성을 확인하기 위해 활용 가 
- 알파 매개변수(하이퍼파라미터)
  - 릿지/라쏘 규제의 양을 조절
  - 알파값이 크면 규제 강도가 강해져 계수값을 더 줄이고 조금 더 과소적합되도록 유도함
  - 알파값이 작으면 계수를 줄이는 역할이 줄어들고 선형회귀 모델과 유사해져 과적합 가능성 높아짐

KNN 주변 이웃의 클래스 비율 = 확률

데이터프레임
- 2차원 표 형식의 주요 데이터 구조
- 통게, 그래프를 위한 매서드 제공
- 넘파이와 상호 변환이 용이하고, 사이킷런과 호환성이 좋음음

로지스틱회귀
- 회귀지만 분류 모델(0 또는 1)
- 선형회귀와 동일하게 선형 방정식을 학습

시그모이드(로지스틱함수)
- 1/(1 + e^(-z))
- z가 무한히큰 음수일 경우 0, 무한히 큰 양수의 경우 1, 0일 경우 0.5 

다중 분류
- 타겟 데이터에 2개 이상클래스가 포함된 분류
- 오름차순 정렬하기 때문에 unique로 출력한 값 순서와 다를 수 있음음

다중분류 수행
- 로지스틱회귀에서는 기본적으로 릿지회귀와 같이 계수의 제곱을 규제 -> L2 규제
- 릿지 = 알파 조정
- 로지스틱회귀 = C 조정
- C의 경우 알파와 반대로 작을수록 규제가 커짐(기본값 1)

소프트맥스(정규화된 지수 함수)
- 다중분류는 이진분류에서 시그모이드 활용과는 달리 소프트맥스를 이용
- 시그모이드 = 하나의 선형방정식의 출력값을 0 ~ 1 사이로 압축
- 소프트맥스 = 여러개의 선형방정식의 출력값을 0 ~ 1 사이로 압축 후 전체 합이 1이 되도록 만듦
- 이를 위해 지수 함수를 사용하기 떄문에 정규화된 지수 함수라고 부름
- e_sum = e^z1 + e^z2 + ... + e^z7
- s1 = e^z1 / e_sum,  s2 = e^z2 / e_sum,  ... , s7 = e^z7 / e_sum
- s1 + s2 + ... + s7 = 1

로지스틱회귀로 확률 예측
- 분류모델은 예측뿐만아니라 예측 근거가 되는 확률 출력 가능 

점진적학습(온라인학습)
- 대표적 알고리즘 -> 확률적 경사 하강법

확률적 경사 하강법
- 전체 샘플을 이용하지 않고 1개의 샘플을 훈련세트에서 랜덤하게 골라 가장 가파른 경사를 찾고 이동
- 위 방법으로 모든 샘플을 모두 사용할 때까지 계속 반복
- 최종적으로 겨냥한 지점에 도달하지 못할 경우 다시 처음부터 반복

에포크(epoch)
- 훈련세트를 한 번 모두 사용하는 과정

미니배치 경사 하강법
- 여러개의 샘플을 사용해 경사 하강법 수행

배치 경사 하강법
- 극단적으로 한 번 경사로를 이동하기 위해 전체 샘플을 사용
- 전체 데이터 사용으로 안정적인 방법이나 자원을 과하게 소비, 데이터를 전부 읽지 못하는 경우도 발생

신경망 알고리즘
- 일반적으로 많은 데이터 사용으로 한번에 모든데이터를 사용하기 어려움
- 모델이 복잡하기 때문에 수학적 방법으로 해답을 얻기 어려움
 -> 확률적/미니배치 경사 하강법 사용

손실 함수(loss function)
- 머신러닝 알고리즘이 얼마나 엉망인지 측정하는 기준 -> 작을수록 좋음
- 샘플 하나에 대한 손실을 정의
- 미분이 가능해야함(연속적 그래프) -> 로지스틱 회귀에서 확률

비용함수(cost function)
- 훈련세트에 있는 모든 샘플에 대한 손실함수의 합
- 통상 비용-손실을 구분하지 않고 사용

로지스틱 손실함수(이진 크로스엔트로피 손실함수)
- 양성 클래스(1) 타겟에는 예측을 그대로 곱하고 음수로 바꿈
 -> 0.9 * 1 = 0.9
 -> -log(에측 확률)
- 음성 클래스(0) 타겟은 그대로 곱했을 경우 무조건 0 이므로 양성처럼 바꾸고 예측도 양성에 대한 예측으로 변환
 -> (1 - 0.2) * 1 = 0.8
 -> -log(1 - 에측 확률)
- 1에 가까운 값을 낮은 손실, 반대의 경우 높은 손실
- 다중 분류의 경우 -> 크로스엔트로피 손실함수

* 에포크 수가 적으면 훈련세트를 덜학습 -> 과소적합 가능성
* 에포크 수가 많으면 훈련세트를 완전학습 -> 과대적합 가능성

조기종료
- 과대적합 시작 전에 훈련을 멈춤


결정트리
- 질문의 결과를 y/n 으로 구분해 직관적 해석
- 질문을 추가해 분류 정확도를 높임
- 전처리 불필요요

노드
- 훈련 데이터의 특성에 대한 테스트를 표현(불리언 값으로 출력)

트리 해석
--------------------
| 테스트 조건      |
| 불순도(gini)     |
| 총 샘플 수       |
| 클래스 별 샘플 수|
--------------------

불순도(gini impurity)
- {1 - (클래스 비율의 제곱합)}
- 지니불순도 = 1 - {(음성 클래스 비율)^2 + (양성 클래스 비율)^2}
- 노드에 하나의 클래스만 있다면 불순도 = 0

정보 이득(information gain)
- 부모와 자식 노드 사이의 불순도 차이
- 정보이득이 최대가 되도록 데이터를 분할

엔트로피 불순도
- 지니 -> 제곱 / 엔트로피 -> 밑이 2인 log
- DecisionTreeClassifier 클래스에서 criterion = 'entropy'(기본값 = 'gini')

가지치기
- 무한히 가지가 생성되는 것을 방지

* 훈련세트에만 잘 맞고 테스트 세트에서 점수가 낮을 때 -> 일반화가 잘 안됨

* 뿌리노드 = 깊이 0 / 리프노드 = max_depth 

검증세트
- 하이퍼파라미터를 튜닝하면서 테스트세트로 결과를 계속 확인하면 테스트세트를 훈련세트처럼 학습해 신뢰도가 떨어짐
- 테스트 세트를 마지막에 한번만 사용하기 위해 훈련세트에서 검증세트를 다시 분리리

교차검증
- 많은 데이터를 훈련에 사용할수록 좋은 모델이 형성되지만 검증세트를 적게 분리하면 검증 데이터 점수에 대한 신뢰도가 떨어짐
- 교차검증을 통해 안정적인 검증 점수를 얻고, 훈련에 많은 데이터 활용
- 검증세트를 분리하고평가하는 과정을 여러번 반복해 얻은 점수를 평균하여 최종점수를 얻음
- 통상 5 또는 10 fold 교차검증 사용용

하이퍼파라미터 튜닝
* AutoML = 사람 개입 없이 하이퍼파라미터 튜닝을 자동으로 수행
- ex) 트리모델에서 최적 max_depth 찾아서 고정시키고 다른 매개벼수를 바꾸면 최적값도 바뀌게 됨
- 그리드 서치
 - 하이퍼파라미터 탐색, 교차검증을 동시에 수행(cross_validate 수행 불필요요)
- 랜덤서치
 - 매개변수 값의 목록을 전달하지 않고 매개변수를 샘플링할 수 있는 확률 분포 객체를 전달
 - 연속된 매개변수 값을 탐색할 때 유용

scipy 라이브러리
- 파이썬 과학 라이브러리
- 적분, 보간, 선형대수, 확률 등을 포함한 수치 계산 전용 라이브러리




# 파이썬 응용 ------------------------------------------------
data = [[a, b] for a,b in zip(a_data, b_data)]      

from scipy.stats imort uniform, ranint
rgen = randint(0, 10)
rgen.rvs(10) # 정수(인덱스 별 추출된 수) 출력 

ugen = uniform(0, 1)
ugen.rvs(10) # 실수(비율) 출력



# 전처리 ------------------------------------------------
from sklearn.model_selection import train_test_split
train_test_split(x_data, y_data, test_size = , stratify = y_data, random_state= ) 
        # 기본 분류 비율 : 25%
        
        # stratify 매개변수에 타겟 데이터를 전달하면 클래스 비율에 맞게 조정
        # 훈련 데이터가 작거나 특정 클래스의 샘플 개수가 적을 때 특히 유용

from sklearn.preprocessing import PolynomialFeatures 
        # 각 특성을 제곱한 항을 추가하고 특성끼리 곱한 항을 추가
        # 가능한 배열 생성
poly = PolynomialFeatures()
poly = PolynomialFeatures(degree = 5, include_bias = False)
        # degree = 5 -> 특성(고차항) 추가
        # include_bias -> 생략해도 됨
poly.fit([[2,3]]) # 리스트 안에 리스트,,
poly.transform([[2,3]])
        # [[2. 3. 4. 6. 9.]] # 2 * 3^2 이건 안됨, 최고 차항에 다른 요소를 곱하지 못한다

from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import Ridge


# pandas ------------------------------------------------
pd.unique(data['speices']) # 열 종류
data.info()
data.describe()


# numpy ------------------------------------------------
fish_target = fish['species'].to_numpy() #-> [] 1차원 배열 생성
fish_input = fish[['weight', 'length', ''']].to_numpy() #-> [[]] 1차원 배열 생성

test_array = np.array([1,2,3,4])
test_array.shape # 행,열 출력
        # [1,2,3,4]
        # (4,) -> 행 요소 4개, 열 1개
test_array.reshape(2, 2) # 재배열
         [[1 2]
          [3 4]]
        # (2, 2) -> 행 요소 2개, 열 2개
test_array.reshape(-1, 1) # -1 = 전체 선
       
np.random.seed(42) # 랜덤 시드 지정
index = np.arange(49) #0 부터 49까지 (n-1)까지 증가하는 배열 생성
np.random.suffle(index) # 주어지는 배열 무작위로 섞음

np.column_stack(([1,2,3],[4,5,6])) # 행-열 변환 / 튜플 형태로 입력
np.column_stack((train_iput**2, train_input)) # 다항회귀를 위한 제곱열을 추가한 배열 생성
        
np.concatenate # 가로 배열

np.mean(trian_input, axis = 0)
np.std(train_input, axis = 0) # 각 열의 평균, 표준편차

np.round(proba, decimals = 4) # decimals > 자릿수 설정

phi = 1 / (1 + np.exp(-z)) # 지수함수 계산(시그모이드)

* 넘파이 배열은 불리언 값을 이용해 행을 선택함





# EDA(시각화) ------------------------------------------------
그래프 x, y축 비율 고려하면서 해석하기, 직관적으로 보이는대로 해석하지 않기
        
plt.scatter(x, y, marker = '^') 
        # marker = '^' or 'D' 모양 지정
plt.scatter(a_data[:, 0], a_data[:, 1]) # 데이터 배열에서 인덱스 0값과 1
        
plt.xlim((0,1000)) # 튜플 형식으로 입력 / plt.ylim((0,1000))

plt.plot([15,50], [15*lr.coef_ + lr.intercept_, 50*lr.coef_ + lr.intercept_])        
  # 선형회귀 직선 그리기

plot_tree(dt, filled = True, feature_names = ['A', 'B', 'C']) 
 -> dt = 결정트리모델, feature_names = 테스트 조


# 모델 매서드 ------------------------------------------------
from sklearn.neighbors import KNeighborsRegressor
knr = KNeighborsRegressor(n_neighbors = 3) # k 조정

kn.score(x_data, y_data)
        # knr에서는 결정계수 값  
kn.predict(x, y) / kn.predict([[30, 600])) # 출력 시 일반 리스트가 아닌 넘파이 배열로 출
  -> array([1])
kn._fit_X
  -> 독립변수 호출             
kn._y
  -> 종속변수 호출
kn.predict_proba(test_scaled[:5]) # 클래스 확률값 반환 > 5개 샘플 확률값
                               
distances, indexes = kn.kneighbors([[25, 150]]) # 최근접 이웃 거리, 이웃 인덱스
        # plt.scatter(train_input[indexes, 0], train_input[indexes, 1], marker = 'D')

from sklearn.linear_model import LogtisticRegression
decisions = lr.decision_function(train_bream_smelt[:5]) # 모델이 학습한 선형 방정식 결과 출력
# coef_ intercept (계수, 절편 출력) <-> decision_function (방정식 결과(z값) 출력)

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(max_depth = 3, random_state =42) -> 최대 깊이 조정

from sklearn.linear_model import SGDClassifier
sc = SGDClassifier(loss = 'log', max_iter = 10, random__state) -> loss = 손실함수 지정, max_iter = epoch 횟수 지정
 * ConvergenceWarning   ->  iter 횟수 재조정 필요

from sklearn.model_selection import cross_validate
scores = cross_validate(dt, train_input, train_target) # 기본 k = 5
# cv = StratifiedKFold -> 분류 / KFold -> 회귀
from sklearn.model_selection import StratifiedKFold
score = cross_validate(dt, train_input, train_target, cv = StratifiedKFold())
splitter = StratifiedKFold(n_splits = 10, shuffle = True, random_state = 42)

from sklearn.model_selection import GridSearchCV
params = {'min_impurity_decrease' : [0.0001, 0.0002, 0.0003, 0.0004, 0.0005]
gs = GridSearchCV(DecisionTreeClassifier(random_state = 42), params, n_jobs = 1)
# 'min_impurity_decrease' 값마다 5-폴드(기본값) 교차검증 수행 -> 5*5 수행
# n_jobs -> 병렬 실행 cpu 코어 수 지정 / 기본값 1 / 모든코어 = -1
gs.fit(train_input, train_target)
gs.best_estimator_   # 검증 점수가 가장 높은 모델
gs.best_params_  # 최적 매개변수
gs.cv_results_['mean_test_score']  # 매개변수에서 수행한 교차검증 평균 점수



# 모델 평가 ------------------------------------------------
from sklearn.metrics import mean_square_error
mae = mean_square_erro(test_target, test_prediction) # 실제값 - 예측값 오차 반환                              





















                               
